## Clean up ggml.c for CRAN compliance
x <- readLines("src/whisper_cpp/ggml.c")
## Clean up ggml.c for CRAN compliance
x <- readLines("src/whisper_cpp/ggml.c")
## Clean up ggml.c for CRAN compliance
x <- readLines("Rimp/src/whisper_cpp/ggml.c")
## Clean up ggml.c for CRAN compliance
x <- readLines("~Rimp/src/whisper_cpp/ggml.c")
## Clean up ggml.c for CRAN compliance
x <- readLines("~/src/whisper_cpp/ggml.c")
## Clean up ggml.c for CRAN compliance
x <- readLines("../src/whisper_cpp/ggml.c")
## Clean up ggml.c for CRAN compliance
x <- readLines("../src/whisper_cpp/ggml.c")
x<-readLines("../src/c/ggml.c")
## Clean up whisper.cpp for CRAN compliance
#x <- readLines("src/whisper_cpp/whisper.cpp")
x<- readLines("../src/cpp/whisper.cpp")
x <- c('#include <Rcpp.h>', x)
## Clean up ggml.c for CRAN compliance
x<-readLines("../src/c/ggml.c")
x
## Clean up whisper.cpp for CRAN compliance
#x <- readLines("src/whisper_cpp/whisper.cpp")
x<- readLines("../src/cpp/whisper.cpp")
x
x <- c('#include <Rcpp.h>', x)
?chr
?nchar()
nchar("sanmi")
nchar("si")
nchar("\U")
nchar("\U00B5")
cat("\U00B5")
?ord()
charToRaw("!")
charToRaw("~")
as.integer("~")
bs <- c(seq(from = charToRaw("!"), to = charToRaw("~")),
seq(from = charToRaw("¡"), to = charToRaw("¬")),
seq(from = charToRaw("®"), to = charToRaw("ÿ")))
charToRaw("ÿ")
charToRaw("®")
charToRaw("¡")
charToRaw("!")
?iconv()
iconv("i")
iconv("!")
byte_to_unicode <- function(x, from = "ISO-8859-1", to = "UTF-8") {
return(iconv(x, from = from, to = to))
}
x <- rawToChar(as.raw(c(72, 101, 108, 108, 111))) # "Hello"
unicode_string <- byte_to_unicode(x, from = "ASCII", to = "UTF-8")
cat(unicode_string)
library(stringi)
# Define a sample text to tokenize
text <- "This is a sample text for BPE tokenization."
# Define the number of BPE operations to perform
num_operations <- 100
# Convert the text to UTF-8 byte sequence
byte_seq <- stri_encode(text, "UTF-8")
# Create a vocabulary of unique UTF-8 bytes
vocab <- unique(byte_seq)
# Convert the vocabulary to a list of Unicode strings
unicode_vocab <- lapply(vocab, function(x) stri_unescape_unicode(paste0("\\u", strtoi(charToRaw(x), base = 16))))
View(unicode_vocab)
# Perform BPE operations on the text
for (i in 1:num_operations) {
# Define a list of possible merges based on the current vocabulary
merge_list <- expand.grid(vocab, vocab)
merge_list <- merge_list[merge_list[, 1] != merge_list[, 2], ]
# Calculate the scores of each merge based on their frequency in the text
scores <- apply(merge_list, 1, function(x) sum(grepl(paste0(x[1], x[2]), byte_seq)))
best_score <- max(scores)
# Find the best merge and update the vocabulary accordingly
if (best_score > 0) {
best_merge <- merge_list[which(scores == best_score)[1], ]
new_seq <- gsub(paste0(best_merge[1], best_merge[2]), paste0(best_merge[1], "_", best_merge[2]), byte_seq)
vocab <- unique(stri_split_fixed(new_seq, "_", simplify = TRUE)[[1]])
byte_seq <- new_seq
} else {
break
}
}
# Convert the final vocabulary to a list of Unicode strings
final_unicode_vocab <- lapply(vocab, function(x) stri_unescape_unicode(paste0("\\u", strtoi(charToRaw(x), base = 16))))
# Return the byte sequence and Unicode string lists
result <- list(byte_seq = byte_seq, unicode_seq = final_unicode_vocab)
charToRaw("d")
charmatch("d")
chartr("d")
rawToChar("d")
charToRaw("1")
charToRaw("!")
iconv("!",from = "c",to ="utf-8")
iconv("!",from = "c",to ="UTF-8")
iconv(33 , from = "latin1",to ="Unicode")
iconv(33 , from = "UTF-8",to ="Unicode")
iconvlist()
iconv(33 , from = "ASCII",to ="Unicode")
